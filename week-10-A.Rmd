---
title: "Week 10, Day 1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(PPBDS.data)
library(stringr)
library(rstanarm)
library(tidyverse)
library(tidymodels)

# We are still working with the kenya data set. In addition to the variables we
# used last week, we will (on Thursday) make use of the county in which the poll
# station was located and of the block_number of that location. Check out the
# stringr code we use to pull those variables out. Can you figure out how the
# **stringr** code below works? Is there a better way to do it?

week_10 <- kenya %>% 
  rename(reg_chg = reg_byrv13) %>% 
  filter(treatment %in% c("control", "local")) %>% 
  droplevels() %>% 
  mutate(poverty_n = (poverty - mean(poverty))/sd(poverty)) %>% 
  mutate(county = str_replace(block, "/\\d*", "")) %>% 
  mutate(block_number = str_extract(block, "/\\d*")) %>% 
  mutate(block_number = str_replace(block_number, "/", "")) %>% 
  select(county, block_number, poll_station, reg_chg, treatment, poverty_n) 

```


## Scene 1

**Prompt:** How do we choose between competing models? First, we need to have a sense of what makes one model "better" than another. There is no single answer, but the most popular approach is to see how well the model's predictions match the truth.


* Fit the same stan_glm() model which we used on Thursday: `reg_chg` as a function of `treatment`, `poverty_n` and their interaction. Look at the results. 

```{r}

fit1 <- stan_glm(reg_chg ~ treatment + poverty_n + poverty_n:treatment, 
         data = week_10,
         refresh = 0)

print(fit1, digits = 4)

rmse(ames_test_res, truth = Sale_Price, estimate = .pred)

```

error is diff between our fitted values and acutal value s

* The root mean square error (also known as RMSE or rmse) is the most common measure for how well a models fits the data. It is the square root of the average of the sum of the residuals squared. (Recall that the residual is defined as the true value minus the fitted value.) Calculate the RMSE by hand. (Hint: Use the `predict()` function with the fitted model object. This will give you the fitted values. Once you have the residual, you just square them, take the sum, and then take the square root of the sum.)


```{r}

try <- predict(fit1)

try %>%
  as.tibble() %>%
  mutate(truth = week_10$reg_chg, forecast = predict(fit1)) %>%
  mutate(sq_diff = (truth - forecast) ^2) %>%
  summarize(rmse = sqrt(mean(sq_diff)))
  
# square root mean between average squared fdfiference between predicted value and real value is .04

```



* Write a sentence or two describing a situation in which RMSE would not be a good metric for choosing among models.

if there are outliers, they can cause significant skewedness, thereby suggesting that the model is a lot worse than it actually is
when you think outliers are really gouing to affect your model because wwith rmse, you squaring you are upping the error

* Write a sentence interpreting sigma. (Recall that, in the Bayesian/stan_glm() framework, sigma is just another parameter, and that the fitted model provides us with an estimated posterior for it.)

Sigma is the error term (difference between the actual value and the estimated value of a parameter)

the median of the posterior distribution for the root mean squared error for the stan_glm model (so 4000 models). this is teh standard deviation of that posterior distribution 


## Scene 2

**Prompt:** Create the same model using the **tidymodels** approach. However, instead of creating a training/test split, and then using the training data for cross-validation, we will just use the whole data at once. This is, after all, what we did above. Hint: Use the Summary from Chapter 10 for guidance: https://davidkane9.github.io/PPBDS/model-choice.html#summary

* Calculate RMSE again by hand. Does it match what you saw above?

* Calculate RMSE using the metrics() argument.


```{r}
fit_2 <- workflow() %>%
  add_recipe(recipe(reg_chg ~ treatment + poverty_n, 
                    data = week_10)) %>%
  step_interact(~ treatment * poverty_n) %>%
  add_model(linear_reg() %>% 
              set_engine("stan"))
```



```{r}
# By hand
fit_2 %>%
  fit(data = week_10) %>%
  predict(week_10) %>%
  bind_cols(week_10 %>% select(reg_chg)) %>% 
  mutate(sq_diff = (.pred - reg_chg) ^ 2) %>%
  summarize(rmse = sqrt(mean(sq_diff)))
# Using metrics function

fit_2 %>%
  fit(data = week_10) %>%
  predict(week_10) %>%
  bind_cols(week_10 %>% select(reg_chg)) %>% 
  metrics(truth = reg_chg, estimate = `.pred`)

```
## Scene 3

**Prompt:** The key problem with this analysis is that we have used the same data to *fit* the model as we are using to *evaluate* the model. This is very, very dangerous. We don't really care how well the model works on data we have already seen. We have the data itself! We care about the future, when we don't know the answer already. *The main purpose of tidymodels is to make it easy to estimate how well our model will work in the future.*

* Create 4 objects: split, train, test and folds data, just as we do in chapter 10.

* Using the same model as in the previous scene, use cross-validation and report the average RMSE over the assessment samples. Hint: `collect_metrics()`.

* Using the same model as in the previous scene, fit the model on the training data and test it on the test data. Hint: `metrics()`. Report the RMSE. Why is it so much lower?



```{r}
set.seed(10)
week10_split <- initial_split(week_10, prob = 0.80)
week10_train <- training(week10_split)
week10_test  <- testing(week10_split)
# 10 folds (samples)

week10_folds <- vfold_cv(week10_train, v = 10)
fit_2 %>%
  fit_resamples(resamples = week10_folds,
                control = control_resamples(save_pred = TRUE)) %>%
  collect_metrics()

## get a sense of how well this data is going to predict our real data set 
```
idea is that splitting is happening randomly, but with seed, we dst it to be the same thing veyr time  

can compare rmse if we were to fit the workflow from fit2 to week10 dataset. doing this on the test dataset

```{r}

stan_wfl %>%
  fit(data = week10_train) %>%
predict(new_data = week10_test) %>%
  bind_cols(week10_test %>% select(reg_chg))


```


